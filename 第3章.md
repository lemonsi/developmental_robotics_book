# 3 新奇、好奇和惊奇

&nbsp;&nbsp;&nbsp;&nbsp;发展型机器人的一个核心原则是自主性，自主性是指发展型机器人与环境的交互和探索是自由的。自主型机器人根据内在状态和对外界环境的感知，来恰当地自主选择自身行为。本章主要关注学习的自主性，也就是智能体自由的选择学什么、何时学和怎么学。
&nbsp;&nbsp;&nbsp;&nbsp;自由的探索会有一个根本问题，什么才是探索环境的最佳策略呢？当机器人面临全新的体验时，它该怎样决定优先选择哪个行为呢，又该在何时选择另一个？我们知道，传统人工智能的解决办法一般是把这个情况看作是一个最优化问题（如能量最小化、探索奖励等），发展型机器人虽然也使用类似的计算工具，但问题的建立和计算策略是不同于这些传统方法的。我们将介绍内在动机的涌现领域，内在动机可以赋予机器人好奇心。内在动机驱动的机器人并非力求解决特定问题，而是学习本身的过程。
&nbsp;&nbsp;&nbsp;&nbsp;人们将内在动机（IM）作为一种驱动自主学习的机制，它不仅应用于发展在机器人学，而且也应用于机器学习领域。相比传统方法，它有三个重要优点。首先，IM是独立于任务的。这意味着可以把机器人放在一个全新的环境中，它没有任何经验，通过自主探索，不仅可以学习到环境的重要特征，而且还能学到应对环境的行为技能。其次，IM促进了技能的层级式学习和再利用。学习的目的，是掌握知识或技能，并非特定的任务。因此IM驱动的机器人可以掌握环境中的一种能力，这种能力可能不会立即带来好处，但可能是后续更多复杂技能的关键基石。最后，IM是开放式的，这样一来，机器人的学习取决于它已经掌握的知识，而不是预先定义的目标。正如我们要强调的，很有IM模型能包含一个原则：当机器人在某个方面的技能已娴熟时，它能将注意力高效地转向环境中的新特征或新技能。
&nbsp;&nbsp;&nbsp;&nbsp;IM的研究受两个密切相关的研究领域的启发：一是在探究IM如何在人类和非人类中发展的问题上有广泛的理论和经验，它们主要是在心理学领域。第二，在神经科学领域也有很多研究成果，这些成果不仅包含IM的神经基础，而且还揭示这些生物机理是如何运作的。
&nbsp;&nbsp;&nbsp;&nbsp;与发展机器人学的其他研究领域相比（比如，运动技能发展和语言学习），IM的研究还处于较早期的阶段，所以IM的研究重点与实验之间没有明确的对应的关系。现在IM的研究重点仍在于设计高效的算法架构，而直接面对人体发展的研究相对少很多。3.3.1节，我们会详细介绍模拟IM的各种架构。另外，迄今进行的大部分工作都是在模拟器上进行研究，这就使得在真实世界机器人平台上获取的数据较少。然而，利用机器人研究IM问题变得越来越热门，本章后半段会用几个例子，来着重介绍IM在模拟器和真实世界的研究。
##3.1 内在动机：概念介绍
&nbsp;&nbsp;&nbsp;&nbsp;虽然心理学的数据和理论对IM概念有重大影响，但是机器学习中的“外在动机”和“内在动机”与心理学的用法并不同。心理学家通常将内在动机行为定义为没有外界刺激，生物体自由选择的动作，而外在动机行为定位为为应对外界提示或引导选择的动作。比如，儿童纯粹为了玩而画画，就是处于内在动机；相反，如果是为了钱或糖果而画画，就是处于外在动机。但在发展型机器人学，我们采纳2011年Baldassarre的观点，即内在动机行为是为了满足基本生理需求，而外在动机是其他的目的。
### 3.1.1早期的影响
&nbsp;&nbsp;&nbsp;&nbsp;早期内在动机方法主要受行为理论的影响。1943年，Hull提出一个知名的自我平衡的理论。这个理论认为，所有行为的产生原因都可以理解为两种：一是原始内驱力，比如饥饿或口渴；二是次要心理内驱力，在满足原始内驱力过程中形成的。Hull的观点有两个关键要素：一原始内驱力是内在的，并受到生物法则的约束，二，它们有一个平衡点，原始内驱力就是为了生物体接近该平衡点而服务的。比如，当动物感到寒冷时，它可能会发抖或移向阳光来增加体温。
&nbsp;&nbsp;&nbsp;&nbsp;有些研究员想知道，Hull的刺激理论是否能解释动物的玩耍和探索行为。1950年，Harlow观察到恒河猴在面对机械谜题时（图3-1）的行为，猴子们大多被谜题所吸引，并且玩法多样。值得注意的是，它们的探索行为并不依赖外在奖励，而且通过反复练习，猴子们解决谜题的效率也逐步提高。由此可知，这些猴子不仅能学习解开谜题，更重要的是，它们对摆弄和探索谜题的尝试似乎也是直接以弄起其原理为目标的。其他研究人员也发现类似的探索行为事件，对这类行为的一种解释是把它与Hull的框架融合起来，也就是说，把探索等行为的激励也看做是一种本能。但是，那些猴子的探索行为并非是想自我平衡，所以本质上和刺激理论无法解释的。因为，它们既不是因为被剥夺食物而产生的焦虑扰动回应，也不是因为有明显的目的或立即的收益。
### 3.1.2 知识与能力
&nbsp;&nbsp;&nbsp;&nbsp;IM之后的解决方法可以分为两个理论派别——基于知识的和基于能力的IM理论。
&nbsp;&nbsp;&nbsp;&nbsp;基于知识的理论认为IM是一种认知机制，它使生物体可以发现新奇的或意外的特性、物体、事件。根据这种观点，IM是生物体当前知识状态的产生的，生物体通过系统地探索环境，寻找不熟悉或不理解的经验，来有目的地拓展知识库。
&nbsp;&nbsp;&nbsp;&nbsp;基于知识的IM包含两类：基于新奇的和基于预测的。基于新奇性的体现了一些发展心理学家提出的理论——经验是被整合进认知结构中的，这种结构可以解释新的信息。当环境中新的特性与现有知识储备不匹配时，就会导致生物体花费努力来解决这种差异。基于新奇性的方法，意味着不同新奇性等级的情境是有关键区别的：低级新奇性对应着熟悉的经验，而高级新奇性不能被生物体当前的知识库解释清楚，而中级新奇性或许适合学习，因为它既可以被理解，又令人觉得陌生。基于预测的IM则注重生物体与环境的互动，生物体在探测环境时，会先预测会事件、对象会对自己的行为做出什么样的反应，以及出现意外结果，如何进一步探索环境。
&nbsp;&nbsp;&nbsp;&nbsp;尽量基于新奇性和基于预测性的IM，都能学习到关于环境的新知识，但两种学习机制有着细微且重要的区别。基于新奇性的IM的学习机制，智能体是相对被动的，因为它寻找新经验的主要方式是在空间中移动，相反基于预测的IM的学习机制，则相对主动，智能体可以由条不紊地在环境里操控并观察结果。其实这种区分是比较随意的，两者并非彼此独立，而是经常在一起出现。
&nbsp;&nbsp;&nbsp;&nbsp;基于知识的IM让智能体专注于如何逐渐弄懂环境的属性。而基于能力的IM，则关注与智能体能力的提升。这里有一些理论，其中1952年Piaget提出了功能仿真现象，它指婴幼儿会有条理的联系新的技能。因此，基于能力的IM的基本含义是，它通过引导智能体寻找挑战的经验来促进能力发展。
### 3.1.3IM的神经基础
&nbsp;&nbsp;&nbsp;&nbsp;IM除了在心理学上的启发，还有在神经科学上的启发。我们简单介绍一下，一些大脑区域的活动是如何与各个类型的IM联系的。
&nbsp;&nbsp;&nbsp;&nbsp;首先，发现新奇性的一个重要区域是海马体，它不仅对长期记忆起着根本性作用，而且还对新物体和新事件的反应处理上也有无可替代的作用。当生物体遇到新体验时，海马体会激活一条位于自身和腹侧被盖区之间的常用通路，通过释放多巴胺在海马体中建立新的记忆痕迹。这种机制，不断在重复事件中进行，直至不再起反应。
&nbsp;&nbsp;&nbsp;&nbsp;其次，有一系列参与感觉运动处理的大脑区域，它们可以为预测学和发现意外事物提供基础。比如，与主动眼球运动相关的额叶眼区（FEF)在视觉搜索中起着关键作用。从猴子的FEF单细胞记录中可知，FEF活动是可预期的（例如，当追踪的物体凭空消失）。此外，如果预期目标的位置和实际观察的位置不一致时，FEF活动就会增加。所以，FEF的神经活动不但可被预测，而且提供了一种学习信号，这种信号可优化未来的感觉运动预测。
&nbsp;&nbsp;&nbsp;&nbsp;最后一个可能和基于能力的IM相关的区域叫作上丘（SC）。2006年Redgrave提出了一个模型，当意外事情出现（比如一道闪光），它能激活SC并导致多巴胺短期释放增多，而且这次多巴胺的增多会强化行为和感觉信号之间的联系，这种联系存在纹状体中。因此SC的作用不仅是生物体行为产生意外结果时发出信号，更重要的是，也增加了这种行为在相似感觉信号来临时发生的可能。
## 3.2内在动机的发展
&nbsp;&nbsp;&nbsp;&nbsp;接下来我们看一下，IM在婴幼儿期是如何发展的。有一点很重要，IM并非独立的研究领域，而是多个交叉研究领域中的某一个部分（如，知觉和认知发展）。下面我们先介绍一下基于知识的IM的成果，然后再着重介绍基于能力的IM。
###  3.2.1 婴儿中基于知识的IM: 新奇性
&nbsp;&nbsp;&nbsp;&nbsp;从行为的角度看，发现新奇的事物需要具备两种能力：一是探索行为——搜索环境中潜在的兴趣区域，二是新奇性探测——发现某个状况是新的。然后把精力集中在那个对象或事件上。
&nbsp;&nbsp;&nbsp;&nbsp;视觉探索是如何发展的？解答这个问题的一种方法，是区分婴儿在自由观察简单几何形状时的扫描模式，我们发现2周大的婴儿，一般注视一个很小的区域，而且视线驻留时间较长。而12周大的婴儿，产生的注视更短更均匀。其他研究也有类似的发现，比如1976年分析两个月左右大的婴儿，观察人脸时的注视模式，年幼点的倾向于专注脸的轮廓，而年长点的可以更好的有条不紊的扫描整张脸，包括眼睛。这种发展型模式可以定义为从内源性到外源性的一个转变。
&nbsp;&nbsp;&nbsp;&nbsp;第二种方式认为发现新物体的核心元素是新奇性检测。这里有两个问题：一是婴儿多大开始能回应新事物。二是怎么能从他们的行为上体现呢。我们可以简化问题，大多数研究给婴儿看视觉形象，测量看的时长，时长的增加或减少，作为衡量新奇性检测的指标。这个研究，研究人员吃惊的发现，0-2个月大的婴儿更倾向于选择熟悉的物体，而对3-6个月的婴儿，他们似乎更倾向于新物体。6-12个月大的婴儿，会稳定的倾向于新物体。有一种假说，是婴儿遇到新物体，需要编码的时间较长，而0-2个月太小的婴儿，由于他们的编码不稳定也不完整，因此更倾向于关注熟悉的物体。根据这个观点，新奇性检测看来是人类一出生就表现出来了。
###  3.2.2 婴儿中基于知识的IM: 预测性
&nbsp;&nbsp;&nbsp;&nbsp;一种完善的衡量婴儿预测行为的技术是1988年Goodman提出的视觉预期范式（VExP)。
&nbsp;&nbsp;&nbsp;&nbsp;他们发现，对在2-4月的婴儿，预测视觉活动不仅在这个阶段快速发展，而且也和单独评估的FEF成熟过程一致。
### 3.2.3 婴儿中基于能力的IM
&nbsp;&nbsp;&nbsp;&nbsp;正如之前提到的，基于能力的IM相比基于知识的IM，更专注于技能的掌握，而不是掌握有关环境的知识。基于能力的IM有一个重要成分——自我效能，它在人类婴儿早期就出现了，是认识到自己的行为对周边物体产生的影响。
&nbsp;&nbsp;&nbsp;&nbsp;婴儿探索自我效能的方一种方法是基于行为的偶然感知。1980年Rovee-Collier设计了一个著名实验，把婴儿放在婴儿床里，用缎带把婴儿的一只腿和头顶的悬挂架系在一起。婴儿很快就学会踢这只连着的移动架的腿。这是一种联合强化模式，移动悬挂架的次数和踢腿次数成正比，也跟着增加。而且婴儿似乎很喜欢这种控制，踢腿时常常咕咕笑。
## 3.3 内驱性智能体和机器人
&nbsp;&nbsp;&nbsp;&nbsp;现在我们把话题转向内驱型智能体和机器人。我们首先阐明如何把IM近似为一个计算问题，然后分别介绍一套模拟基于知识和基于能力的IM基本架构。最后我们将深入探讨这些架构是如何应用到各种模拟、模型以及真实环境的机器人平台上的。
### 3.3.1 IM的计算框架
&nbsp;&nbsp;&nbsp;&nbsp;目前发展型机器人IM的大部分研究方法都倾向于使用强化学习（RL）。正如Barto等人在2004年提出的那样，RL提供了一种多样的学习框架，不仅包含环境（外部动机）如何影响行为，还包含了内部因素（内在动机）如何影响行为。
&nbsp;&nbsp;&nbsp;&nbsp;RL的核心元素包含自主智能体、环境、体验环境的感觉状态、每种状态可能的行为。
#### 基于知识的IM:新奇性
&nbsp;&nbsp;&nbsp;&nbsp;让智能体寻找新奇事件的简单直接的方法是，首先给它一个函数\
$$
P(e^k,t)，它表示时刻t观察到事件e^k的概率估计。
$$
&nbsp;&nbsp;&nbsp;&nbsp;我们可以假设函数初始时时均匀分布的，随着经验的累加来调整函数P，然后我们给定P函数和常数C，就可以得到一个随事件可能性减小而增加的奖励函数。
$$
r(e^k, t) = C*(1-P(e^k, t))
$$
&nbsp;&nbsp;&nbsp;&nbsp;把这个奖励函数嵌套在常规RL问题中时，智能体会倾向于选择罕见事件的行为。这个公式有一个问题，会导致不可能的事件也将获得最大回报，后面我们再讨论这个问题。
&nbsp;&nbsp;&nbsp;&nbsp;2007年Oudeyer把上面的奖励函数描述为不确定动机，智能体由内在激励驱动去寻找新奇事件。另一种可行的计算方式，是信息增益动机，奖励函数定义为增加自身知识的可观察事件。具体来说，我们先将H(E,t)定义为在t时刻所有事件E的熵，即：
$$
H(E,t) = -ΣP(e^k,t)ln(P(e^k, t))
$$
&nbsp;&nbsp;&nbsp;&nbsp;而奖励函数则与信息增益和常数C相关
$$
r(e^k, t) = C(H(E,t) - H(E,t+1))
$$
&nbsp;&nbsp;&nbsp;&nbsp;这两种奖励函数起到的效果不同，不确定动机将IM于环境中事件的发生概率联系在一起，而信息增益动机一个优点在于IM可以随智能体知识状态的函数而变化。
#### 基于知识的IM:预测
&nbsp;&nbsp;&nbsp;&nbsp;基于知识的IM预测和基于知识的IM新奇，思想是类似的。但知识代价函数与预测偏差有关，具体是当预测偏差越大，代价函数越高，但这会使机器人尝试极端预测不低的情况，所以也有人设置了最大偏差，如果预测偏差大于这个最大偏差，则代价函数值会变小。具体函数形式为：
$$
r(SM(->t)) = C1*e^-C*|Er(t)-E_r^θ|^2
$$
#### 基于能力的IM
&nbsp;&nbsp;&nbsp;&nbsp;基于能力的IM也和基于知识IM思想类似，但代价函数与能力提升有关，有一种代价函数被称为最大化能力进步, 具体函数形式为：
$$
r(SM(->t), g_k, t_g) = C*(l_a(g_k,t_g - Θ)-l_a(g_k,t_g))
$$
这里tg-θ指前一阶段的尝试。
### 3.3.2 基于知识的IM:新奇性
&nbsp;&nbsp;&nbsp;&nbsp;之前介绍的IM架构，都还只是理想的抽象IM，并没有经过系统的评估和比较。而接下来我们将介绍最近几年这些架构在模拟或真实世界机器人的应用成果。我们先从基于新奇性的IM说起吧。
&nbsp;&nbsp;&nbsp;&nbsp;前面介绍过，寻找新奇性的行为可以细分为两部分：探索和新奇性检测。2007年Vieira-Neto等人提出了一种整合者两种元素的模型。探索了移动机器人了的视觉探索和习惯。大概的原理是：首先，让机器人使用基本的避障策略来探索环境，在漫游环境时，机器人会得到由视觉输入引起的显著性映射，然后，映射中突出的位置会进一步分析，这些位置都要通过新奇性滤波器处理，这个滤波器具体的作用是：将输入的颜色值投影到特征的自组织映射中，而习惯机制会通过调节权值，逐渐减小这些值。
&nbsp;&nbsp;&nbsp;&nbsp;下图展示了机器人刚开始探索环境时的输入情况，标号表示突出位置（0号最突出），每个位置的圈表示新奇性滤波器的输出。我看可以看到，新奇性滤波器刚开始输出值很大，但随着时间延长而逐渐减小，当机器人第5次穿过该环境时，由于它已经熟悉了区别的两边和地面，新奇滤波器的输出会维持在很低的水平。
&nbsp;&nbsp;&nbsp;&nbsp;下图说明了机器人如何应对对放入环境中的新物体。它碰到红球时，在球的位置（标签0、4、5）会产生三个很大的值，导致新奇性滤波器进一步处理这些位置，然后新奇性滤波器的输出陡增。有一个细节需要说明，本实验习惯机制在新奇性阶段是关闭的，以便球的新奇性在于机器人的几次相遇后可以估算出来。虽然这个模型不完全适合婴幼儿视觉的探索和学习，但是它简明的揭示了某种行为机制（如避障）是如何驱动视觉探索的，也说明了环境中的新特性是如何被探测识别并进一步视觉处理的。
&nbsp;&nbsp;&nbsp;&nbsp;2002年Huang等也提出了相关的方法。他在SAIL(自组织、自主、增量学习者)移动机器人平台上，考察新奇性和习惯。与Vieira-Neta使用视觉突出来定义新奇性不同，Huang等人的模型用期望的和观察的状态差异来定义新奇性，该模型使用了多模态的信号（视觉、听觉、触觉），该模型的另一个重要特性是在RL框架内实现新奇性探测和习惯机制。它是怎么做的呢？该模型包含内部信号和外部信号：内部信号由感觉新奇性发出，而外部信号由老师（人）发出，老师可以通过按机器人身上的“好/坏”按键给机器人奖励和惩罚。
&nbsp;&nbsp;&nbsp;&nbsp;我们来看模型的认知结构。感知输入传入IHDR树（增量分层决策树），IHDR树估计当前环境并更新环境模型，同时对当前感觉状态分类，然后模型根据环境、状态、价值评估选择行为，然后，利用Q-Learning算法更新感觉状态、行为和Q值。

> 这是强化学习框架，根据状态、环境、

### 3.3.3 基于知识的IM:预测性
&nbsp;&nbsp;&nbsp;&nbsp;
### 3.3.4 基于能力的IM
&nbsp;&nbsp;&nbsp;&nbsp;
## 3 .4 本章总结
&nbsp;&nbsp;&nbsp;&nbsp;


